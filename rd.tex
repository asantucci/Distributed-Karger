\documentclass[12pt]{article} 

\usepackage{algorithm2e}
\usepackage{amsthm}
\usepackage{amsmath}

\newtheorem{theorem}{Theorem}

\begin{document} 

\title{\textbf{Distributed MST and Min Cut}}
\author{Eric Lax, Andreas Santucci}
\date{June 3rd, 2015}
\maketitle

\section{Overview}

We set out to solve the problem of finding a min cut of a large graph $G$, where the number of vertices fits on a single machine but the number of edges does not. In this paper, we will briefly outline previous approaches to this problem, and then explore two avenues. 

The first is a Karger variant, which finds the min cut with very high probability. To do this, we develop a distributed minimum spanning tree algorithm to simulate each iteration of Karger's algorithm. The run-time of finding the MST is $O(\frac{m \log(\log(n))}{B})$. The shuffle cost associated with finding MST is $O(n \log(n))$. In order to accomplish Karger's objective, this must be run $O(n^2 \log(n))$ times. 

The second approach is an approximation algorithm based on the intuition that when sampling a graph $G$, the cut that is most likely to be disconnected is the min cut. We go through three different approaches to solving this problem, each with a different trade-off: one requires a higher communication cost, one requires a higher run-time, and one requires more machines.

The first variant requires a high number of machines, but can be run in $O(\frac{\log^2(n)\log(\log(n))}{\epsilon^2})$. The second variant has a higher shuffle cost of $O(\frac{\log(n)}{\epsilon^2}(n \log(n) + Bn))$.

Below we will show that these approaches yield the fastest min cut approximation under the assumptions that $O(\frac{n\log(n)}{\epsilon^2})$ do not fit on a single machine. Moreover, even for small graphs, if $\epsilon$ is large enough, these approaches will perform at least as well as current approximation algorithms. Finally, we will explore further areas of research, including potential improvements to our algorithm from effective resistance, and applying the same methodology to the S-T min cut problem.

\section{Literature Review}

\textbf{To do: citations}

\paragraph{Karger} In David Karger's 1993 paper, ``Global Min-cuts in RNC and other Ramifications of a Simple Mincut algorithm'', he developed an $O(m n^2 \log(n))$ algorithm which finds the min cut with high probability. The algorithm repeatedly contracts edges at random until only one cut remains. This process is repeated $O(n^2 \log(n)$ times. It was later improved upon in the Karger-Stein algorithm \cite{Karger, David R; Stein, Clifford (1996). ``A new approach to the minimum cut problem''} to run in $O(n^2 \log^3(n))$ time.

\paragraph{Skeleton Graphs} In his 1999 paper, ``Minimum Cuts in Near-Linear Time'', Karger develops a general methodology computing a skeleton graph, computing a tree packing on the resulting skeleton graph, and then calculating cut sizes determined by trees in the packing. This algorithm finds the min cut in $O(m \log^3(n))$ time.

\paragraph{Random Sampling Techniques} In his 2000 paper, ``Random Sampling in Cut, Flow, and Network Design Problems'', Karger turns this approach into a $(1+\epsilon)$ approximation which runs in $O(m + n(\frac{\log(n)}{\epsilon})^3)$ time. This approach has been repeatedly adapted and improved upon. The most recent and best approximation algorithm we could find to date was published by Danupon Nanongkai and Hsin-Hao Su in a paper entitled, ``Almost-Tight Distributed Minimum Cut Algorithms'', which achieves a $(1 + \epsilon)$ approximation in $O((\sqrt{n} \log^*(n) + D) \epsilon^{-5} \log^3(n))$ time, where $D$ denotes the diameter of the graph, and $\log^*(n)$ denotes the iterated logarithm. Although the algorithm is a distributed method, it does not consider the cases where the skeleton graph does not fit on a single machine.

\paragraph*{Karger Variants} In the 2011 paper, ``Filtering: A Method for Solving Graph Problems in MapReduce'', Lattanzi, Moseley, Suri, and Vassilvitskii is the only paper we were able to find which attempts to find min cuts for large graphs which don't fit on a single machine. \textbf{To do: run time analysis}. Their general approach is to arbitrarily weight edges in the range $(0,1)$, find a threshold $t \in (0,1)$ such that sampling all edges with weight less than $t$ results in a sub-graph small enough to fit on a single machine. They then apply Karger-Stein's algorithm. The problem with this approach is that a connected components analysis is simultaneously run for many values of $t$ to find the optimal value of $t$, leading both to extremely high communication cost and machine usage.

\section{Karger Variant}

\paragraph{Relating MST and Karger}
Due to the cut property of MST's, the lowest weight edge leaving each connected component must be contained in the minimum spanning tree. Prim's algorithm starts with each node as its own connected component, the lowest weight outgoing edge is found, and used to extend the connected component. Each iteration across all connected components reduces the number of connected components by at least $1/2$, therefore there are at most $\log(n)$ rounds required. 

\textbf{To do: Cite Prim's}

\subsection{Algorithm}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[H]
\caption{Distributed Karger}
\SetKwProg{Map}{Map}{:}{}
\SetKwProg{Reduce}{Reduce}{:}{}
\SetKwProg{Broadcast}{Broadcast}{:}{}
\SetKwProg{Partition}{Partition}{:}{}
\Partition{}{Place all edges for each node one a single machine}
Set min-cut to $\infty$ \\
\For{i = 1 to $n^2 \log(n)$} {
  Assign arbitrary weights \\
  Distributed MST \\
  Remove maximum weighted edge (We now have a cut) \\
  Perform Connected Components Analysis to determine how nodes are partitioned \\
  Create a hash set for each side of the partition, containing the nodes on that side \\
  \Map{}{
    Across edges, check for edge between two partitions using the two hash-sets \\
  }
  \Reduce{} {
    Return number of edges between two partitions
  }
  \If{smallest cut thus far} {
    update min cut
  }
}
Return min-cut
\end{algorithm}

\subsubsection{Distributed MST Approach - Low Run Time}

\paragraph{General Approach}
Our general approach is fairly simple. Since in our Karger algorithm, edges are already partitioned such that all edges from a vertex are stored on a single machine. To start, consider each node as its own connected component. We perform a map to find the lowest weight edge from each node, and then a reduce step to find the lowest weight edge leaving each connected component. Results are sent back to the driver. 
The driver then locally run's Prim's algorithm to combine connected components. The lowest weight edge leaving each connected component is then broadcast out to all machines\footnote{It is worth noting that we can reduce the communication cost and run-time by caching the lowest weight edge for each node on the driver in the event that still leaves the connected component formed after running Prim's. In the next iteration, we don't have to check for that nodes lowest weight edge, and this also means less communication.
}, at which point each machine locally combines connected components. 

Another map is performed finding the lowest weight edge leaving each connected component from each node, and another reduce step is used to find the lowest weight edge leaving each connected component. These results are sent back to the driver. Prim's algorithm is again executed locally, and the process is continued until either there is one connected component or there are no more outgoing edges leaving connected components.

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[H]
\caption{Distributed MST (low run time)}
\SetKwProg{Map}{Map}{:}{}
\SetKwProg{Reduce}{Reduce}{:}{}
\SetKwProg{Broadcast}{Broadcast}{:}{}
\SetKwProg{Partition}{Partition}{:}{}
Set each node as a connected component \\
Assign arbitrary weights to all edges (uniformly at random) \\
\While{Edges exist leaving connected components} {
  \Map{}{
    For each node, find the lowest weight edge leaving the connected component which contains that node \\
  }
  \Reduce{}{
    Find the lowest weight edge leaving each connected component
  }
  \Broadcast{}{Lowest weight edge leaving each connected component to all machines}
  On each machine, merge connected components together, and in the process create a hash set for each connected component storing nodes belonging to the component
}
Return MST
\end{algorithm}

\subsection*{Algorithm Analysis}

\paragraph{Arbitrary Edge Weights}
Since we just draw numbers uniformly at random to create edge weights, the cost of this operation is $O(m/B)$.

\paragraph{Finding Lowest Weight Edge Leaving Each Node}

Finding the lowest weight edge from each node requires checking each edge, which costs $O(\frac{m}{B})$. When iterating over edges in the form of $(i,j)$, we check if node $j$ is in the same connected component as $i$ in constant time using the hash set containing associated with the connected component for $i$. This process is explained in detail below. If $j$ is not contained in the set, we know the edge leaves the connected component. This lets us track whether the edge $(i,j)$ is the lowest weight edge leaving the connected component thus far. Since there are $\log(n)$ iterations, the total cost of this operation across iterations is $O(\frac{m \log(n)}{B})$.

\paragraph{Finding Lowest Weight Edge Leaving Each Component}

The reduce step requires an all to one communication of $O(n)$, since each node sends back the lowest weight edge. To actually find the lowest weight edge leaving each connected component is upper bounded by $O(n)$.  In total, the shuffle size is $O(n \log(n))$.

\paragraph{Broadcasting Connected Components}
In each step, we have a list of edges which we wish to add to our connected components to combine them. There is one edge per connected component. We will broadcast this to machines such that connected components may be updated locally.

Since the number of connected components is reduced by $1/2$ on each iteration, the first broadcast requires shuffle size $n$, the second requires $n/2$, the third requires $n/4$. In general, on each iteration, the shuffle cost of the broadcast is $2^{-i}n$, where $i$ denotes the iteration number.

Notice that the size of the broadcast is $O(n)$. Since it is broadcast to $B$ machines, the shuffle size is $O(Bn)$.

\paragraph{Merging Connected Components} 

In order to merge connected components together efficiently and keep track of which nodes belong to each component, we require the use of several data structures. We first create a hash map where the keys consist of the unique node id's, and the values consist of pointers to hash sets. When the lowest weight edge leaving each connected component is broadcast, for each edge $(i,j)$, we use our hash map to look up in constant time both the hash set containing $i$, and the hash set containing $j$. We then merge these two hash sets, by adding all the elements from the smaller hash set to the larger hash set. We update the pointers accordingly for the nodes belonging to the smaller hash set. The smaller hash set is then discarded and no longer used.

This process is deterministic. So, the same hash-sets and maps will appear on each machine and the driver.

It's worth noting that we will only move $O(n)$ elements in each iteration. Since adding an element to a hash set is $O(1)$, this process runs in $O(n)$ time. Similarly, we are updating maximally $O(n)$ pointers, each pointer can also be update in $O(1)$ time. Therefore, merging connected components costs $O(n)$. Since there are at most $\log(n)$ iterations required in order to form a minimum spanning tree, the cost of merging connected components is total $O(n \log(n))$.

\paragraph{Run Time Analysis - Distributed MST (low run time)}

Arbitrary weighting costs $O(m/B)$. Finding the lowest weight edge from each node is $O(\frac{m \log(n)}{B})$. Finding the lowest weight edge from each connected component is $O(n)$. Merging connected components is $O(n \log(n))$. So, total run time is given by $O(\frac{m \log(n)}{B} + n \log(n))$.

\paragraph{Shuffle Size - Distributed MST (low run time)}

Finding the lowest weight edge leaving each connected component requires shuffle size $O(n \log(n))$. Broadcasting the connected components costs $O(Bn)$. The total shuffle size is given by $O(n\log(n) + Bn)$.


\subsubsection{Distributed MST Approach - Low Number Map Reduces}

Our general approach is fairly simple. We first find the lowest weight $\log(n)$ edges leaving each vertex. Since in our Karger algorithm, edges are already partitioned such that all edges from a vertex are stored on a single machine. Using a max-heap data structure of size limited to $\log(n)$, we iterate through all $\text{deg}(v_i)$ edges leaving node $i$, and for each, check to see if its smaller than the max element in $O(1)$ time. If it is, we add it to the heap which costs $O(\log(\log(n)))$ time, and remove the max. If it's bigger, we continue to the next item. We then sort the heap into an array in $O(\log(n) \log(\log(n)))$ time. The total run time across all vertices is given by

\[
\frac{\sum_{i} \text{deg}(v_i) \log(\log(n))}{B} = \frac{2m \log(\log(n))}{B}
\]

Therefore the run time is $O(\frac{m \log(\log(n))}{B})$.

We then perform an all-to-one communication and send the results back to the driver, with shuffle size $O(n \log(n))$. The driver then checks for the lowest weight edge leaving each connected component. In order to do this, the driver simply checks the lowest weight edge leaving each node, meaning the driver will check $n$ nodes. Since the $\log(n)$ edges are already sorted, each node can be checked in constant time, so this takes a total $O(n)$ time each iteration. There are a total of $\log(n)$ iterations, since each iteration reduces the number of connected components by a factor of at least two. 

\paragraph{Caveat} 

Note, however, that we must also deal with the case where all $\log(n)$ edges leaving a single node are exhausted in the search for an MST, but we have more than one connected component remaining. It is then possible that the minimum weight edge leaving that connected component does not reside within the driver. Therefore, we no longer attempt to augment this connected component with any of the remaining outgoing edges from other nodes within the connected component. If this becomes true of all our connected components, we can no longer add edges to reduce the number of components. 

This is very unlikely, and will not occur for most graphs. Notice that in order for this to happen, our smallest connected component would have to be of size at least $\log(n)$ (probably much larger). However, if this event does occur, we repeat the process described above. 

Since each connected component is at least size $\log(n)$, there are at most $n/ \log(n)$ connected components; the driver can fit $\log(n)^2$ edges leaving each connected component, since $n/\log(n) \cdot \log(n)^2 = n \log(n)$. The size of the broadcast is $n$, since all we need to know is which connected component contains each node. So we may broadcast the list of connected components back to all machines. 

We repeat the process above, considering the next lowest weight $\log(n)$ edges leaving each connected component. The results are sent back to the driver, with a shuffle size of $n \log (n)$. Notice that the smallest connected component we could have before running out of outgoing edges on this iteration would be of size $\log(n)^2$. 

In the worst case, the number of iterations performed for this process is given by $\log^x(n) \geq n$, such we that have removed at least $n-1$ connected components. We then have

\[
x = \frac{\log(n)}{\log(\log(n))}
\]

Although this is a lower total number of iterations, the shuffle size per iteration is high enough to make our previous algorithm more efficient in the worst case. However, since in applying Karger's Variant the edge weights are assigned randomly, the worst case is unlikely to occur. In future research, we plan to delve more deeply into finding the expected run time given random assignment of edge weights. Given that we do not have time to do this, however, we implemented the first algorithm. Our Karger run-time analysis makes use of the low run time algorithm as well.

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[H]
\caption{Distributed MST (low map-reduces)}
\SetKwProg{Map}{Map}{:}{}
\SetKwProg{Reduce}{Reduce}{:}{}
\SetKwProg{Broadcast}{Broadcast}{:}{}
\SetKwProg{Partition}{Partition}{:}{}
Set each node as a connected component \\
Assign arbitrary weights to all edges (uniformly at random) \\
\While{Edges exist leaving connected components} {
  \Map{}{
    Across each connected components: Search for lowest $log(n)$
    weight edges leaving \emph{each} connected component
  }
  Send lowest weight $\log(n)$ results back to driver \\
  \While{True} {
    \If{One connected component remains \textbf{Or} At least one 
    node in each connected component has exhausted all $\log(n)$ edges} {
    break
    }
    \For{Each Connected Component such that none of the nodes have exhausted all $\log(n)$ outgoing edges} {
      Take minimum edge leaving component and use this to merge two
      components
    }
    \Broadcast{}{Connected Components to all machines}
  }
}
Return MST
\end{algorithm}

\paragraph{Run Time Analysis (per iteration) - Distributed MST (low map-reduces)}

Computing the minimum spanning tree first requires finding the lowest weight $\log(n)$ edges leaving each vertex. To do this, we use a max-heap data structure, taking $O(\frac{m\log(\log(n))}{B})$ time. Computing the MST on the driver takes $O(n \log(n))$ time. The total run time is given by $O(\frac{m \log(\log(n))}{B} + n\log(n))$.

\paragraph{Shuffle Size (per iteration) - Distributed MST (low map-reduces)}

The algorithm only uses one all to one communication, since the edges have already been partitioned as a first step in the variant of Karger's algorithm. For each node, the lowest weight $\log(n)$ edges are sent back, so on each iteration the shuffle cost is given by $O(n\log(n))$.

\subsection{Algorithm Analysis}

\paragraph{Run Time Analysis - Karger Variant (using low-run time MST)}

Notice that in the algorithm, after weighting edges randomly and finding an MST, the largest weight edge is removed and connected components analysis is performed to determine how nodes are partitioned across the cut. The connected components analysis is $O(n)$ since there are $O(n)$ edges, and can be performed using a simple Prim's algorithm. While performing connected components analysis, we accumulate a hash-set of nodes stored in each side of the partition in a similar fashion to the method described for the low-run time MST. We then map across edges, checking to see if the edge spans the cut. Using the hash-set, each edge requires a constant time check, so the total cost when computed in parallel is given by $O(m/B)$.

Since computing MST is the bottleneck, and since we need $n^2 \log(n)$ iterations to run Karger, the total run-time is given by \\
$O((n^2 \log(n)) (\frac{m \log(n)}{B} + n \log(n)))$. Note that for $B \geq \log(n)$, this is faster than Karger's algorithm.

\paragraph{Shuffle Size - Karger Variant (using low-run time MST)}

We have an all to all communication required by partitioning edges. The cost of this operation is $O(m)$. We also shuffle the lowest weight $\log(n)$ edges from each node, which is an all to one communication of size $n \log(n)$. The map-reduce to get the size of each cut is an all to one communication of size $O(B)$, since each machine simply returns the number of edges crossing each cut.

Both these operations are performed $n^2\log(n)$ times, so the total shuffle cost is given by $O(n^2\log(n) (n \log(n)))$. 

\paragraph{Limitations in Scaling}
Note that we require that each machine can store at least $O(n)$ information, so for graphs large enough, this becomes a problem.

\paragraph{How strongly does Algorithm scale} Although this algorithm beats Karger under certain conditions, it is far from optimal. Even with a large number of machines, we still have a $n\log(n)$ term incurred from MST, and we require $n^2 \log(n)$ iterations. As the number of machines grow drastically, it would be possible to adopt a version of parallel Prim's that would result in faster run time at the expense of increased shuffle size.

\section{Distributed Min Cut Approximation Algorithm}

Let $c$ denote the size of the true min cut of $G$. First, we are going to sample each edge with probability $p$. Let $G'$ be the sub-graph from sampling edges in $G$ with probability $p$. We are going to choose $p$ such that the expected sub-graph which emerges will be disconnected and fit on one machine. 

Let us examine a simple fully connected graph with nodes $A, B, C, D$.  In $G'$, the probability that nodes $A$ and $B$ are disconnected from $C$ and $D$ is a function of the cut-size $AB$, defined as $\xi$. More precisely, the probability that they are disconnected in $G'$ equals $(1-p)^\xi$, since no edge that crosses that cut can be chosen to be in our sub-graph. The larger the cut, the less likely it is to be disconnected in $G'$. By definition, the min-cut is most likely to be disconnected. If we generate enough random sub-graphs $G'$, the cut which is most often disconnected will have size less than $(1+\epsilon) \cdot c$ with high probability.

Let us define an $\alpha$ cut as a cut whose size is less than or equal to $\alpha c$. From (Karger 1993), the number of $\alpha$-cuts is less than $n^{2 \alpha}$. Let $f(\alpha)$ be the number of cuts of size $\alpha c$.

Let $Z(\alpha) = \# \text{ times a cut of size } \alpha c \text{ is disconnected}$, $y = \# \text{ of iterations}$, and let $X$ denote the probability the min-cut is disconnected, which is given by $(1 - p)^c$. Let $\mu = E[Z(\alpha)] = yx^\alpha$.

\begin{theorem} It's highly improbable that cuts of size $\alpha c$, for $\alpha \geq 3$, will be disconnected most often if we have more than $\frac{4}{\sqrt{2}} \cdot 2 \cdot 3 \cdot (1-\epsilon)^2 \ln{(n)}$ iterations.
\end{theorem}

\begin{proof}
Recognize that $Z(1)$ is the number of times the min-cut is disconnected. Then, using a Chernoff bound, 

\begin{align*}
\Pr \bigg(Z(1) < (1-\epsilon) \mu \bigg) &\leq e^{(-\mu \epsilon^2)/2} \\
&\leq e^{(-xy \epsilon^2)/2} \\
\end{align*}

If $y \geq \frac{1}{x} \ln(n) \cdot \frac{1}{2\epsilon^2}$, then $\Pr \bigg(Z(1) < (1-\epsilon)\mu \bigg) \leq e^{-\ln(n)} = n^{-1}$. So we may say this event is highly unlikely. 

Let $\Pr(\alpha) = \Pr \bigg( Z(\alpha) > Z(1) \bigg)$. From (Karger 1993) the probability that any cut is disconnected more often than $Z(1)$ is simply given by the sum of $\sum_{\alpha} \Pr(\alpha) f(\alpha)$ where $f(\alpha)$ denotes the number of cuts of size $\alpha$. Define $F(x) = \sum_{\alpha \leq x} f(x)$, where $F(x) \leq n^{2x}$. Taking the worst case scenario where $F(x) = n^{2x}, \forall x$, we can further relax $F(x)$ to be a real-valued function rather than restricting it to the space of integers, in which case $f(a) = dF/d\alpha$, we can then take the integral

\[
\int_{1}^\infty \Pr(\alpha) \frac{dF}{d\alpha} d\alpha
\]

Since it is highly unlikely that $Z(1)$ is less than $(1-\epsilon)xy$, then $\Pr(\alpha) \leq \Pr \bigg(Z(\alpha) > (1-\epsilon)xy \bigg)$. We may upper bound the probability that a cut is more often disconnected than the min-cut. For $\alpha \geq 3$:

\begin{align*}
\Pr(Z > (1 - \epsilon)yx) &= \Pr(Z - \mu > (1 - \epsilon)yx - \mu) \\
&\leq \Pr(|z-\mu| > (1-\epsilon)yx - yx^\alpha) \\
&= \Pr \bigg(|z-\mu| > \mu ((1-\epsilon) x^{-(\alpha-1)}-1)\bigg) \\
&\leq \exp \bigg[-yx^\alpha \bigg((1-\epsilon)yx^{-(\alpha-1)}-1\bigg)^2/3\bigg] \\
&\approx \exp \bigg[-yx^\alpha \bigg((1-\epsilon)x^{-(\alpha - 1)}\bigg)^2/3\bigg] \\
&= \exp \bigg[-y (1-\epsilon)^2 x^{-(\alpha-2)}/3\bigg]
\end{align*}

Now, let $y = \frac{4}{\sqrt{2}} \cdot 2 \cdot 3 \cdot (1-\epsilon)^2 \ln{(n)}$, then

\begin{align*}
&\int_{3}^\infty n^{2\alpha}\cdot \ln{(n^2)} \Pr(\alpha) d\alpha \\
= &\int_{3}^\infty n^{2\alpha} \ln{(n^2)} \exp \bigg[-y x^{-\alpha + 2} \cdot (1-\epsilon)^2/3 \bigg] d\alpha \\
= &\int_{3}^\infty n^{2\alpha} \ln{(n^2)} \exp \bigg[-\ln{n}\bigg(\frac{1}{(1-\epsilon)^2}\bigg) \cdot \frac{4}{\sqrt{2}} \cdot 6x^{-\alpha + 2} \frac{(1-\epsilon)^2}{3} \bigg] d\alpha \\
= &\int_{3}^\infty n^{2\alpha} \ln{(n^2)} n^{-(\frac{4}{\sqrt{2}} \cdot 2x^{-\alpha + 2})} d\alpha \\
= &\int_{3}^\infty n^{2 (\alpha - \frac{4}{\sqrt{2}} \cdot \frac{1}{x^{\alpha-2}})} \ln{(n)}  d\alpha \hspace{55pt} \text{We may non-problematically choose } x < \frac{1}{\sqrt{2}} \\
\leq &\int_{3}^\infty n^{-2\alpha} \ln{(n^2)} d\alpha \\
\leq &\frac{1}{n^6}
\end{align*}
\end{proof}

So clearly for $\alpha \geq 3$, we can disregard the possibility that a cut of size $\alpha c$ is disconnected the most.

\begin{theorem}
For $\alpha < 3$, the probability that there exists a cut of size $\alpha$  such that $Z(\alpha) > (1+\epsilon) E[Z]$ is minimal. 
\end{theorem}

\begin{proof}
Let $y = \frac{6 \ln(n)}{x^3 \epsilon^2}$. Define $\text{Pr}_1(\alpha) = \Pr(Z(\alpha) > (1+\epsilon) E[Z(\alpha)])$

Using the logic from above, the probability the theorem holds for all $\alpha < 3$ is given by $\int_{1}^3 \text{Pr}_1(\alpha) \frac{dF}{d \alpha} d\alpha$

\begin{align*}
\text{Pr}_1(\alpha) = \Pr(z > \mu(1 + \epsilon)) = \exp \bigg[ -\mu \frac{\epsilon^2}{3}\bigg] = \exp \bigg[-y x^\alpha \cdot \frac{\epsilon^2}{3} \bigg]
\end{align*}

Now taking the integral, and plugging in for $y$ and $\frac{dF}{d \alpha}$, we get a value less than $\frac{1}{n}$.

\begin{align*}
\int_{1}^3 \text{Pr}_1(\alpha) \frac{dF}{d \alpha} d\alpha = \int_{1}^3 \exp \bigg[-y x^\alpha \cdot \frac{\epsilon^2}{3} \bigg] \frac{dF}{d \alpha} d\alpha &\leq \frac{1}{n}
\end{align*}

\end{proof}

\begin{theorem}
With high probability, the cut which is disconnected most frequently will have a cut size within $(1+\epsilon)$ of the true min-cut $c$. Let $y = \frac{6 \ln(n)}{x^3 (\epsilon/2)^2}$, and let $x = \frac{1}{e}$.
\end{theorem}

\begin{proof}
By theorem 1 we can ignore the case when $\alpha \geq 3$. For $\alpha < 3$, we know from theorem 2 that it's highly probable that $Z(\alpha) < (1+\frac{\epsilon}{2})yx^\alpha$ (we divide $\epsilon$ by $2$ based on our choice for $y$).
Similarly, we know that $Z(1) > (1-\frac{\epsilon}{2}) yx$ with high probability. Further, if $Z(\alpha) > Z(1)$, then it's highly probable that $yx^\alpha > (1-\epsilon) yx$, so solving for $\alpha$ we get $\alpha = 1 + \frac{\ln(1-\epsilon)}{\ln(x)}$. Substituting in for $x = \frac{1}{e}$ we see that $\alpha = 1 - \ln(1 - \epsilon) \approx 1+\epsilon$. Note that for lower values of $x$ the approximation is closer to $1$.
\end{proof}

\section{Distributed Min Cut Approximation Algorithm}

\subsection{Binary Search}

When sampling sub-graphs from $G$, we want the expected number of components to be less than or equal to $\log(n)$, such that when counting the cuts between connected components, i.e. disconnected cuts, a full enumeration is $2^{\log(n)} = O(n)$. We seek to perform a binary search to find optimal $p$ such that in expectation at least half of the time we sample $G'$, we produce less than or equal to $\log(n)$ connected components. We can take some constant number of samples and run connected components to get a sense of the expected number of connected components. Start with some initial $p_1 = \frac{1}{\delta}$, where $\delta$ denotes the min degree of the graph. Sample edges with probability $p_1$ to yield graph $G'_1$, run connected components, count number of connected components. Repeat $x$ times, where $x$ defined as above. Then average the counts of connected components across $x$ iterations to yield an estimate for the expected number of connected components. If greater than $\log(n)$, guess again $p_2 = \frac{1}{2} \cdot p_1$. If expected number is more than 1 and less than $\log(n)$, we're done. Else, equal to 1, then $p_2 = 2 \cdot p_1$. Repeat as necessary. Maximally $\log(n)$ iterations, so run time is $O(n \log (n))$. Note: in order to parallelize this, we may run $p_1, \frac{1}{2}p_1, \frac{1}{4}p_1$ until we run out of machines under the assumption that our initial guess yields more than $\log(n)$ connected components in expectation. The required number of iterations is then $O(\frac{\log(n)}{B})$.

\paragraph{Choosing x}

Note in our proof we put an upper bound on $x$, however, this is not problematic for two reasons. First, the proof works out in the general case for $\alpha < kx^\alpha$ for some $k \in \mathbb{R}$. For the sake of simplifying notation, we used a constant bound. The upper bound on $x$, simply provides a lower bound on $p$. Since low $p$ correlates to a higher number of connected components on the resulting sub-graph, it does not interfere with our binary search.

In fact, the upper bound for $x$ we set of $1/e$ was attained by setting $p = 1/c$, which we approximate by $1/\text{min degree}$ as the place to start our binary search.

\subsection{Algorithm}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[H]
\caption{Distributed Min Cut}
Binary Search for optimal $p$ \\
\SetKwProg{IP}{In Parallel}{:}{end}
\IP{}
{
  Sample each edge with probability $p$ \\
  Compute connected components on sub-graph \\
  Return connected components \\
}
Count number of times each cut is disconnected \\
Maximally occurring cut is highly likely to be a $1+\epsilon$ approximation of min cut
\end{algorithm}


\subsection{Run Time and Shuffle Size Analysis}

The run time varies drastically depending on the methodology used to sample connected components. The binary search require sampling $\log(n)$ sub-graphs, where each sample costs \textbf{Find Karger's sampling methodology}. Let $CC$ denote the time it takes to run connected components analysis. We then have to run $CC \log(n)$ connected components analyses to determine optimal $p$. Once we have $p$, we must sample another $\log(n)/\epsilon^2$ graphs, and perform yet another $CC \log(n)/\epsilon^2$ connected components analysis. Enumerating the number of disconnected cuts takes $O(n)$ with a carefully chosen $p$. Thus, finding the most often disconnected cut from these lists is upper bounded by $O(n\log(n)/\epsilon^2)$.

\subsubsection{High Machines}
In Karger's 1997 paper, ``Fast Connected Components Algorithms for the EREW PRAM'', he develops a fast connected components analysis which runs in $O(\log(n) \log(\log(n)))$ time, but which requires $(m + n)/\log(n)$ EREW processors (exclusive read, exclusive write). We recognize that this may not be feasible for larger graphs, however, if those resources are available, we perform a binary search for $p$ such that $E[\text{\# connected components}] = \log(\log^2(n))$. Therefore enumeration of all disconnected cuts only takes $O(\log^2(n))$. Thus, the whole analysis is $O(\frac{\log^2(n) \log(\log(n))}{\epsilon^2})$, as the bottleneck is sampling and computing sub-graphs.

\subsubsection{High Shuffle}

In the low-run time MST described above, the run-time cost for finding an MST, which yields connected components, is $O(\frac{m \log(n)}{B} + B\log(n))$ with shuffle size $O(n\log(n) + Bn)$. In this case, we chose $p$ such that $E[\text{\# connected components}] = \log(n)$, in which case again counting disconnected cuts is $O(n)$. The bottleneck still falls upon sampling sub-graphs. The cost of the whole analysis is $O(\frac{\log(n)}{\epsilon^2} (\frac{m \log(n)}{B} + B \log(n)))$ The shuffle for each iteration costs $O(n\log(n) + Bn)$ so total shuffle for the algorithm is $O(\frac{\log(n)}{\epsilon^2}(n\log(n) + Bn))$.

Note that there also exists a Pregel framework for computing connected components, which has a high run-time. This is discussed in the paper, ``Finding Connected Components in Map-Reduce in Logarithmic Rounds''.

\section{Conclusion} In conclusion, with high number of machines, our algorithm becomes exceedingly fast. Even in the absence of high machines, our process works regardless of the size of the sub-graphs, thus forming a new approach to approximation algorithms. It's worth noting, that although we've dealt with graphs in an un-weighted context, we should be able to handle weights by sampling not with constant $p$ but with $p_i$ inversely proportional to each edges weight. More research into binary search in this case.

\section{Areas Future Research}

If we find an efficient methodology from going from a list of connected components to the most often disconnected cut without requiring full enumeration, it is possible to remove the binary search from this process, which frees up more flexibility on $p$, which allows us to sample graphs such that they are small enough to fit locally on a single machine.

Moreover, another interesting avenue of research is to sample edges with respect to effective resistances. Thus far, we know that sampling with respect to effective resistances makes the min-cut comparatively more likely to be disconnected, which should reduce the number of iterations required. However, determining a tight-bound will be left for future research.

Finally, the same approach generalizes to ST Min-Cut, if we can find a way to guarantee that $S$ and $T$ are separated a constant proportion of the time when sampling sub-graphs. The general approach to this methodology would be to perform a binary search for $p$ such that $S$ and $T$ are separated $1/2$ the time, and then only consider sub-graphs where they are disconnected.



\end{document}